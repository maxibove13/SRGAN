{"cells":[{"cell_type":"markdown","metadata":{"id":"y3Oa6cU4xMDN"},"source":["# Super Resolution GAN (SRGAN) implementation"]},{"cell_type":"markdown","metadata":{"id":"UrC-T3yWxMDV"},"source":["### Mount drive if in colab"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wc_FXsfVxMDX"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Navigate to repository\n","%cd /content/drive/MyDrive/Github/SRGAN\n","\n","!pip install albumentations==0.4.6"]},{"cell_type":"markdown","metadata":{"id":"42ipoOxnxMDa"},"source":["### Import needed modules"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7dYPb1QIxMDb"},"outputs":[],"source":["import config\n","import torch\n","from torch import nn\n","# Optimization algorithms\n","import torch.optim as optim\n","# Dataset manager\n","from torch.utils.data import DataLoader\n","\n","from torchvision.models import vgg19"]},{"cell_type":"markdown","metadata":{"id":"osNBARtpxMDc"},"source":["## 0. Define and prepare data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bffE2TXsxMDd"},"outputs":[],"source":["from dataset import MyImageFolder\n","\n","dataset = MyImageFolder(root_dir=\"new_data\")\n","print(f\"{len(dataset)} samples in dir {dataset.root_dir}/{dataset.class_names[0]}\")"]},{"cell_type":"markdown","metadata":{"id":"CB3dlJtBxMDf"},"source":["## 1. Create model"]},{"cell_type":"markdown","metadata":{"id":"aAv7qG5RxMDh"},"source":["### Building blocks (Convolutional, Upsample, Residual)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wgXl8jUkxMDi"},"outputs":[],"source":["class ConvBlock(nn.Module):\n","    # Conv -> BN (Batch Norm) -> Leaky/PReLu\n","    def __init__(\n","        self,\n","        in_channels,\n","        out_channels,\n","        discriminator=False,\n","        use_act=True,\n","        use_bn=True,\n","        **kwargs,\n","    ):\n","        super().__init__()\n","        self.use_act = use_act\n","        self.cnn = nn.Conv2d(in_channels, out_channels, **kwargs, bias=not use_bn)\n","        self.bn = nn.BatchNorm2d(out_channels) if use_bn else nn.Identity()\n","        self.act = (\n","            nn.LeakyReLU(0.2, inplace=True) if discriminator else nn.PReLU(num_parameters=out_channels)\n","        )\n","    \n","    def forward(self, x):\n","        return self.act(self.bn(self.cnn(x))) if self.use_act else self.bn(self.cnn(x))\n","\n","class UpsampleBlock(nn.Module):\n","    def __init__(self, in_c, scale_factor):\n","        super().__init__()\n","        self.conv = nn.Conv2d(in_c, in_c * scale_factor ** 2, 3, 1, 1)\n","        self.ps = nn.PixelShuffle(scale_factor) # in_c *4, H, W, --> in_c, H*2, W*2\n","        self.act = nn.PReLU(num_parameters=in_c)\n","\n","    def forward(self, x):\n","        return self.act(self.ps(self.conv(x)))\n","\n","class ResidualBlock(nn.Module):\n","    def __init__(self, in_channels):\n","        super().__init__()\n","        self.block1 = ConvBlock(\n","            in_channels,\n","            in_channels,\n","            kernel_size = 3,\n","            stride = 1,\n","            padding = 1\n","        )\n","        self.block2 = ConvBlock(\n","            in_channels,\n","            in_channels,\n","            kernel_size = 3,\n","            stride = 1,\n","            padding = 1,\n","            use_act = False\n","        )\n","\n","    def forward(self, x):\n","        out = self.block1(x)\n","        out = self.block2(out)\n","        return out + x"]},{"cell_type":"markdown","metadata":{"id":"K1oLcsnsxMDl"},"source":["### Generator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PzAutFivxMDm"},"outputs":[],"source":["class Generator(nn.Module):\n","    def __init__(self, in_channels=3, num_channels=64, num_blocks=16):\n","        super().__init__()\n","        self.initial = ConvBlock(in_channels, num_channels, kernel_size=9, stride=1, padding=4, use_bn=False)\n","        self.residuals = nn.Sequential(*[ResidualBlock(num_channels) for _ in range(num_blocks)])\n","        self.convblock = ConvBlock(num_channels, num_channels, kernel_size=3, stride=1, padding=1, use_act=False)\n","        self.upsamples = nn.Sequential(UpsampleBlock(num_channels, 2), UpsampleBlock(num_channels, 2))\n","        self.final = nn.Conv2d(num_channels, in_channels, kernel_size=9, stride=1, padding=4)\n","\n","    def forward(self, x):\n","        initial = self.initial(x)\n","        x = self.residuals(initial)\n","        x = self.convblock(x) + initial\n","        x = self.upsamples(x)\n","        return torch.tanh(self.final(x))"]},{"cell_type":"markdown","metadata":{"id":"rzKxaMAxxMDn"},"source":["### Discriminator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h3wxUa4WxMDo"},"outputs":[],"source":["class Discriminator(nn.Module):\n","    def __init__(self, in_channels=3, features=[64, 64, 128, 128, 256, 256, 512, 512]):\n","        super().__init__()\n","        blocks = []\n","        for idx, feature in enumerate(features):\n","            blocks.append(\n","                ConvBlock(\n","                    in_channels,\n","                    feature,\n","                    kernel_size = 3,\n","                    stride = 1 + idx % 2,\n","                    padding = 1,\n","                    discriminator = True,\n","                    use_act = True,\n","                    use_bn = False if idx == 0 else True\n","                )\n","            )\n","            in_channels = feature\n","\n","        self.blocks = nn.Sequential(*blocks)\n","        self.classifier = nn.Sequential(\n","            nn.AdaptiveAvgPool2d((6,6)),\n","            nn.Flatten(),\n","            nn.Linear(512*6*6, 1024),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Linear(1024, 1)\n","        )\n","\n","    def forward(self, x):\n","        x = self.blocks(x)\n","        return self.classifier(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qQ--nOr6xMDp"},"outputs":[],"source":["def test():\n","    low_resolution = 24\n","    with torch.cuda.amp.autocast():\n","        x = torch.randn((5, 3, low_resolution, low_resolution))\n","        gen = Generator()\n","        gen_out = gen(x)\n","        disc = Discriminator()\n","        disc_out = disc(gen_out)\n","\n","        print(gen_out.shape)\n","        print(disc_out.shape)\n","\n","test()"]},{"cell_type":"markdown","metadata":{"id":"XVBJ5ixmxMDq"},"source":["### Initialize Generator and Discriminator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OaqBaXARxMDr"},"outputs":[],"source":["gen = Generator(in_channels=3).to(config.DEVICE)\n","disc = Discriminator(in_channels=3).to(config.DEVICE)"]},{"cell_type":"markdown","metadata":{"id":"IvdhMNQuxMDr"},"source":["## 2. Loss and optimizer"]},{"cell_type":"markdown","metadata":{"id":"bFmJKuvyxMDs"},"source":["### Loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9VxZ-79BxMDs"},"outputs":[],"source":["class VGGLoss(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.vgg = vgg19(pretrained=True).features[:36].eval().to(config.DEVICE)\n","        self.loss = nn.MSELoss()\n","\n","        for param in self.vgg.parameters():\n","            param.requires_grad = False\n","\n","    def forward(self, input, target):\n","        vgg_input_features = self.vgg(input)\n","        vgg_target_features = self.vgg(input)\n","        return self.loss(vgg_input_features, vgg_target_features)\n","\n","bce = nn.BCEWithLogitsLoss()\n","vgg_loss_fun = VGGLoss()\n","mse = nn.MSELoss()"]},{"cell_type":"markdown","metadata":{"id":"CgMxkPJ6xMDt"},"source":["### Optimizers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b7MxdstcxMDt"},"outputs":[],"source":["opt_gen = optim.Adam(gen.parameters(), lr=config.LEARNING_RATE, betas=(0.9, 0.999))\n","opt_disc = optim.Adam(disc.parameters(), lr=config.LEARNING_RATE, betas=(0.9, 0.999))"]},{"cell_type":"markdown","metadata":{"id":"WnPpRqrnxMDu"},"source":["## 3. Training"]},{"cell_type":"markdown","metadata":{"id":"JhXfglv1xMDv"},"source":["### Train discriminator and generator functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bcmhXhhkxMDv"},"outputs":[],"source":["### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n","def train_discriminator(D, opt, fake, high_res, bce):\n","\n","    # Reset gradients to zero\n","    opt.zero_grad()\n","\n","    # Train on real data\n","    pred_real = D(high_res)\n","    loss_real = bce(pred_real, torch.ones_like(pred_real) - 0.1 * torch.rand_like(pred_real))\n","\n","    # Train on fake data\n","    pred_fake = D(fake.detach())\n","    loss_fake = bce(pred_fake, torch.zeros_like(pred_fake))\n","\n","    loss = loss_real + loss_fake\n","\n","    # Backward pass\n","    loss.backward()\n","    # Update weights\n","    opt.step()\n","\n","    return loss\n","\n","### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n","def train_generator(D, opt, fake, high_res, vgg_loss, mse):\n","\n","    # Reset gradients to zero\n","    opt.zero_grad()\n","\n","    pred_fake = D(fake)\n","\n","    adv_loss = 1e-3 * bce(pred_fake, torch.ones_like(pred_fake))\n","    vgg_loss = 0.006 * vgg_loss(fake, high_res)\n","    mse_loss = mse(fake, high_res)\n","\n","    loss = adv_loss + vgg_loss + mse_loss\n","\n","    # Backward pass\n","    loss.backward()\n","    # Update weights\n","    opt.step()\n","\n","    return loss"]},{"cell_type":"markdown","metadata":{"id":"XRUeIpZ-xMDw"},"source":["### Load data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZA7rsMo0xMDw"},"outputs":[],"source":["loader = DataLoader(dataset, batch_size=config.BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=config.NUM_WORKERS)"]},{"cell_type":"markdown","metadata":{"id":"rvLjCHl2xMDw"},"source":["### Load last checkpoint"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9JCxvtbVxMDx"},"outputs":[],"source":["from utils import load_checkpoint\n","\n","if config.LOAD_MODEL:\n","    load_checkpoint(\n","        config.CHECKPOINT_GEN,\n","        gen,\n","        opt_gen,\n","        config.LEARNING_RATE\n","    )\n","    load_checkpoint(\n","        config.CHECKPOINT_DISC, disc, opt_disc, config.LEARNING_RATE\n","    )"]},{"cell_type":"markdown","metadata":{"id":"_uUvJJvlxMDx"},"source":["### Training loop"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"eX8P92zNxMDy"},"outputs":[],"source":["from utils import plot_examples, save_checkpoint, plot_loss\n","from time import process_time\n","import time\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from IPython import display\n","from matplotlib.ticker import MaxNLocator\n","\n","print(f\"SRGAN training: \\n\")\n","print(f\" Total training samples: {len(dataset)}\\n Number of epochs: {config.NUM_EPOCHS}\\n Mini batch size: {config.BATCH_SIZE}\\n Number of batches: {len(loader)}\\n Learning rate: {config.LEARNING_RATE}\\n\")\n","\n","\n","loss_disc = []\n","loss_gen = []\n","\n","# Start the stopwatch\n","t0 = process_time()\n","\n","fig, ax = plt.subplots(figsize=(10,6), dpi= 80)\n","\n","for epoch in range(config.NUM_EPOCHS):\n","    for idx, (low_res, high_res) in enumerate(loader):\n","\n","        # Send images to device\n","        high_res = high_res.to(config.DEVICE)\n","        low_res = low_res.to(config.DEVICE)\n","\n","        # Generate fake (high_res) image from low_res\n","        fake = gen(low_res)\n","\n","        loss_disc_e = train_discriminator(disc, opt_disc, fake, high_res, bce)\n","        loss_gen_e = train_generator(disc, opt_gen, fake, high_res, vgg_loss_fun, mse)\n","\n","        # At the end of every epoch\n","        if idx == config.BATCH_SIZE-1:\n","\n","\n","            # Plot gen and disc loss\n","            # Append current epoch loss to list of losses\n","            loss_disc.append(float(loss_disc_e.detach().cpu()))\n","            loss_gen.append(float(loss_gen_e.detach().cpu()))\n","\n","            x = np.arange(0, epoch+1)\n","            print(x, loss_disc, loss_gen)\n","            ax.plot(x, loss_disc, label='Discriminator loss', marker='o', color='b')\n","            ax.plot(x, loss_gen, label='Generator loss', marker='o', color='r')\n","            ax.set_title('Evolution of losses through epochs')\n","            ax.set(xlabel='epochs')\n","            ax.set(ylabel='loss')\n","            # ax.set_xlim(left=0, right=config.NUM_EPOCHS-1)\n","            ax.grid()\n","            if epoch == 0:\n","              ax.legend(loc='upper right')\n","            ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n","\n","            display.clear_output(wait=True)\n","            print(f\"SRGAN training: \\n\")\n","            print(f\" Total training samples: {len(dataset)}\\n Number of epochs: {config.NUM_EPOCHS}\\n Mini batch size: {config.BATCH_SIZE}\\n Number of batches: {len(loader)}\\n Learning rate: {config.LEARNING_RATE}\\n\")\n","            # Display current figure\n","            display.display(fig)\n","            # Pause execution 0.1s\n","            time.sleep(0.1)\n","            plt.close()\n","            ax.grid()\n","            \n","            # Print progress every epoch\n","            print( \n","                f\"Epoch [{epoch}/{config.NUM_EPOCHS} - \"\n","                f\"Loss D: {loss_disc_e:.4f}, Loss G: {loss_gen_e:.4f}]\"\n","                )\n","            # Test generator in an test image\n","            plot_examples(\"low_res/\", gen, epoch)\n","\n","    if config.SAVE_MODEL:\n","        save_checkpoint(gen, opt_gen, filename=config.CHECKPOINT_GEN)\n","        save_checkpoint(disc, opt_disc, filename=config.CHECKPOINT_DISC)\n","\n","# Stop the stopwatch\n","t1 = process_time()\n","print(f\"Elapsed time: {t1-t0}\")\n","\n","plt.savefig(\"loss_evol.png\")"]},{"cell_type":"code","source":["import os\n","\n","test_images = next(os.walk(\"datasets/testing/\"))[2]"],"metadata":{"id":"NTHB8h5BZW1w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Test generator with pretrained net (checkpoint)"],"metadata":{"id":"7i5BuFpHWWLX"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_37fqIEquqmF"},"outputs":[],"source":["import os\n","\n","from utils import load_checkpoint, plot_examples\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","\n","testing = True # If not testing: validation\n","r = 3 # Zoom factor\n","\n","# Initialize SRGAN Generator\n","gen = Generator(in_channels=3).to(config.DEVICE)\n","# Define optimizer for Generator\n","opt_gen = optim.Adam(gen.parameters(), lr=config.LEARNING_RATE, betas=(0.9, 0.999))\n","\n","# Load checkpoint (w&b of specified training)\n","if config.LOAD_MODEL:\n","    load_checkpoint(\n","        config.CHECKPOINT_GEN,\n","        gen,\n","        opt_gen,\n","        config.LEARNING_RATE\n","    )\n","    # load_checkpoint(\n","    #     config.CHECKPOINT_DISC, disc, opt_disc, config.LEARNING_RATE\n","    # )\n","\n","# Get all images in testing folder\n","\n","# Test generator in all images in testing folder\n","plot_examples(\"datasets/testing/\", gen, 0)\n","\n","# Get list of generated super resolution images\n","sr_images = next(os.walk(\"datasets/testing/sr/\"))[2]\n","\n","# Get list of testing images\n","test_images = next(os.walk(\"datasets/testing/\"))[2]\n","\n","# If validating, get hr images\n","if not testing:\n","  hr_images = next(os.walk(\"new_data/hr/\"))[2:][0]\n","\n","# Loop through all test_images\n","for idx, im in enumerate(test_images):\n","    # Read lr, sr and hr images\n","    lr_im = mpimg.imread(f\"datasets/testing/{im}\")\n","    sr_im = mpimg.imread(f\"datasets/testing/sr/{sr_images[idx]}\")\n","    if not testing:\n","      hr_im = mpimg.imread(f\"new_data/hr/{hr_images[idx]}\")\n","\n","    # Get new widths and heights given zoom in factor r\n","    w0 = sr_im.shape[0]//2-sr_im.shape[0]//r\n","    w1 = sr_im.shape[0]//2+sr_im.shape[0]//r\n","    h1 = sr_im.shape[1]//2-sr_im.shape[1]//r\n","    h2 = sr_im.shape[1]//2+sr_im.shape[1]//r\n","    w0_l = lr_im.shape[0]//2-lr_im.shape[0]//r\n","    w1_l = lr_im.shape[0]//2+lr_im.shape[0]//r\n","    h1_l = lr_im.shape[1]//2-lr_im.shape[1]//r\n","    h2_l = lr_im.shape[1]//2+lr_im.shape[1]//r\n","\n","    # Crop images and define titles for comparison figure\n","    ims = [lr_im[w0_l:w1_l, h1_l:h2_l, :], sr_im[w0:w1, h1:h2, :]]\n","    titles = [\"Low Resolution\", \"Super Resolution\"]\n","    if not testing:\n","      ims.append(hr_im[w0:w1, h1:h2, :])\n","      titles.append('High Resolution')\n","\n","    # Initialize figure and axes\n","    fig, axs = plt.subplots(1,2) if testing else plt.subplots(1,3)\n","    \n","    # show image in each subplot, set title deactivate axis\n","    for idx, ax in enumerate(ims):\n","      axs[idx].imshow(ax)\n","      axs[idx].set_title(titles[idx], fontsize=36)\n","      axs[idx].axis('off')\n","\n","    # Set size and save figure\n","    fig.set_size_inches((40, 40), forward=False)\n","    fig.savefig(f\"figures/test_{im}.png\", bbox_inches='tight')"]},{"cell_type":"code","source":["!git status"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-W8BLyRqw0CO","executionInfo":{"status":"ok","timestamp":1645908161885,"user_tz":180,"elapsed":10053,"user":{"displayName":"Maximiliano Bove","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_rx44Dcty8WSCK5hbIbnHSlV4-RQ7IZUT7hQknQ=s64","userId":"16535384186793535256"}},"outputId":"c468f307-464c-4d89-f603-ba94906a621b"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["On branch main\n","Your branch is up to date with 'origin/main'.\n","\n","Changes not staged for commit:\n","  (use \"git add <file>...\" to update what will be committed)\n","  (use \"git checkout -- <file>...\" to discard changes in working directory)\n","\n","\t\u001b[31mmodified:   .gitignore\u001b[m\n","\t\u001b[31mmodified:   config.py\u001b[m\n","\t\u001b[31mmodified:   srgan.ipynb\u001b[m\n","\t\u001b[31mmodified:   utils.py\u001b[m\n","\n","Untracked files:\n","  (use \"git add <file>...\" to include in what will be committed)\n","\n","\t\u001b[31mcomparison.png\u001b[m\n","\n","no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["42ipoOxnxMDa","osNBARtpxMDc","CB3dlJtBxMDf","aAv7qG5RxMDh","rzKxaMAxxMDn","IvdhMNQuxMDr","bFmJKuvyxMDs","CgMxkPJ6xMDt","WnPpRqrnxMDu","JhXfglv1xMDv","XRUeIpZ-xMDw","rvLjCHl2xMDw","_uUvJJvlxMDx"],"name":"srgan.ipynb","provenance":[]},"interpreter":{"hash":"2018421ac86191517594f147876dbf7cc1c30e03728eafb7def7b7c31a8e41ae"},"kernelspec":{"display_name":"Python 3.8.10 64-bit ('.venv_srgan': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}